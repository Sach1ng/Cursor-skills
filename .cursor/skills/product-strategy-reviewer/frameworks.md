# Strategy Frameworks Reference

Detailed frameworks for deeper product strategy reviews.

## Jobs to Be Done (JTBD)

**Format**: When I [situation], I want to [motivation], so I can [outcome].

**Review questions**:
- Is the job functional, emotional, or social?
- Is this a core job or a related job?
- What's the struggling moment that triggers the job?
- What are the hiring and firing criteria for solutions?

**Red flags**:
- Solution described as the job ("Users want a dashboard")
- No struggling moment identified
- Job is too broad to be actionable

## Design for Delight (D4D)

**Three levels**:
1. **Functional**: Does the basic job
2. **Emotional**: Makes users feel good
3. **Social**: Users share/recommend

**Review questions**:
- What's the "delighter" beyond functional?
- Is there a "can't go back" moment?
- What creates the emotional connection?
- Why would users tell others about this?

## Opportunity Sizing

**Components to verify**:
- **TAM**: Total addressable market
- **SAM**: Serviceable addressable market
- **SOM**: Serviceable obtainable market (realistic target)

**Red flags**:
- Only TAM mentioned (inflated opportunity)
- No bottoms-up validation
- "If we get just 1% of the market..."

## Competitive Analysis Depth

**Levels of analysis**:
1. **Feature comparison** (table stakes)
2. **Positioning comparison** (how they're perceived)
3. **Business model comparison** (how they make money)
4. **Strategic intent comparison** (where they're heading)

**Review questions**:
- Is the comparison honest about where competitors are ahead?
- Are indirect competitors included (different solution, same job)?
- What's the sustainable advantage, not just current advantage?

## Risk Assessment

**Categories to check**:
- **Execution risk**: Can we build this?
- **Market risk**: Will customers want this?
- **Competitive risk**: Will others beat us to it?
- **Technical risk**: Is the technology ready?
- **Business model risk**: Can we make money?

**Each risk should have**:
- Likelihood (high/medium/low)
- Impact (high/medium/low)
- Mitigation plan

## Metrics Quality Check

**SMART criteria**:
- **Specific**: Exactly what are we measuring?
- **Measurable**: How will we collect this data?
- **Achievable**: Is the target realistic?
- **Relevant**: Does this metric matter for the goal?
- **Time-bound**: By when?

**Metric anti-patterns**:
- Vanity metrics (look good, don't drive decisions)
- Lagging indicators only (no leading indicators)
- Easily gamed metrics
- Too many metrics (no focus)

## Pre-Mortem Template

Ask: "It's 6 months from now, and this failed. What happened?"

**Common failure modes to check**:
- Customer didn't want it (validation failure)
- Couldn't build it in time (execution failure)
- Competitor beat us (market timing failure)
- Couldn't scale it (technical failure)
- Couldn't monetize it (business model failure)
- Team burned out (resource failure)
- Stakeholders pulled support (political failure)

## Strategy Quality Rubric

| Dimension | Weak | Strong |
|-----------|------|--------|
| **Customer focus** | Vague segments, assumed needs | Specific segments, validated needs |
| **Problem clarity** | Symptoms addressed | Root cause addressed |
| **Solution fit** | Features listed | Outcomes described |
| **Differentiation** | "Better" claims | Unique value proposition |
| **Scope** | Everything included | Explicit trade-offs |
| **Metrics** | Vague or vanity | SMART and meaningful |
| **Risks** | Not mentioned | Identified with mitigations |
| **Execution** | Aspirational timeline | Capacity-grounded plan |
